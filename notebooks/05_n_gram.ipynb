{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9e397b-2007-4bd3-993d-6a4838d60c6e",
   "metadata": {},
   "source": [
    "# N-Gram Model Implementation\n",
    "\n",
    "**Definition**  \n",
    "An n-gram is a sequence of *n* consecutive tokens (words).  \n",
    "The model approximates the probability of the next token as:  \n",
    "\n",
    "$$\n",
    "P(w_t \\mid w_{t-n+1}^{t-1}) \\approx \n",
    "\\frac{\\text{count}(w_{t-n+1}, \\dots, w_{t})}\n",
    "     {\\text{count}(w_{t-n+1}, \\dots, w_{t-1})}\n",
    "$$\n",
    "\n",
    "**Example (n=1,2,3)**  \n",
    "Sentence: `\"i love machine learning\"`  \n",
    "- unigram (*n=1*): `i` → predicts `love`  \n",
    "- bigram (*n=2*): `i love` → predicts `machine`  \n",
    "- trigram (*n=3*): `i love machine` → predicts `learning`  \n",
    "\n",
    "**Estimation**  \n",
    "We predict the next token by estimating its probability from counts in the training data:  \n",
    "\n",
    "$$\n",
    "P(\\text{learning} \\mid \\text{machine}) = \n",
    "\\frac{\\text{count}(\\text{machine, learning})}{\\text{count}(\\text{machine})}\n",
    "$$\n",
    "\n",
    "**Corpus example**  \n",
    "[\"i love machine learning\", \"machine learning is awesome\", \"this machine is awesome\"]\n",
    "\n",
    "- counts:  \n",
    "  - `(\"machine\", \"learning\") = 2`  \n",
    "  - `(\"machine\", \"is\") = 1`  \n",
    "  - `(\"machine\") = 3`  \n",
    "\n",
    "$$\n",
    "P(\\text{learning} \\mid \\text{machine}) = \\tfrac{2}{3}, \\quad\n",
    "P(\\text{is} \\mid \\text{machine}) = \\tfrac{1}{3}\n",
    "$$\n",
    "\n",
    "Therefore, the model predicts **`learning`** after `\"machine\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e37e65",
   "metadata": {},
   "source": [
    "### Step 1 - Tokenize\n",
    "\n",
    "Before building n-grams, we must split text into tokens (usually words).  \n",
    "\n",
    "**Example**: `\"The cat sat.\"` → `[\"the\", \"cat\", \"sat\"]`  \n",
    "\n",
    "To implement this we use regex:\n",
    "- `\\b`: start or end of a word\n",
    "- `\\w`: one word character [a-z] [A-Z] [0-9] [_]\n",
    "- `\\w+`: multiple word characters\n",
    "\n",
    "We also normalize (e.g., lowercase) so counts are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c86966-0f40-4795-b286-dd81cbcf3c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'machine', 'learning']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def tokenize(text:str) -> List[str]:\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "print(tokenize(\"I love machine learning!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c124d71-d01e-4428-b219-ddc6ba8a0c58",
   "metadata": {},
   "source": [
    "### Step 2 - Build Vocabulary\n",
    "\n",
    "The vocabulary is the set of tokens the model knows. We build the vocab by collecting all tokens from the training data. We also add a `unk` token to handle unseen tokens later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf800502-90a4-40bb-b6fe-e86de67c3f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this', 'learning', '<unk>', 'is', 'i', 'machine', 'love', 'awesome'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Set, Tuple\n",
    "\n",
    "def build_vocab(texts: List[str]) -> Set[str]:\n",
    "    vocab = {\"<unk>\"}\n",
    "    for text in texts:\n",
    "        vocab.update(tokenize(text))\n",
    "    return vocab\n",
    "\n",
    "corpus = [\"I love machine learning.\", \"Machine learning is Awesome!\", \"This machine is awesome.\"]\n",
    "\n",
    "vocab = build_vocab(corpus)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376b834-1282-4cf3-a1b8-370327855b27",
   "metadata": {},
   "source": [
    "### Step 3 - Map unkown tokens\n",
    "\n",
    "When we apply the model to new text, we may see tokens that are not in our vocab. To handle this, we replace them with the `unk` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23e7982d-5a1e-4d3f-9926-bee5a9f071e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "def map_unk(tokens: List[str], vocab: Set[str]) -> List[str]:\n",
    "    mapped_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab: \n",
    "            mapped_tokens.append(\"<unk>\")\n",
    "        else:\n",
    "            mapped_tokens.append(token)\n",
    "    return mapped_tokens\n",
    "\n",
    "text_new_vocab = \"I love icecream.\"\n",
    "tokens = tokenize(text_new_vocab)\n",
    "print(map_unk(tokens, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981b42f8-5979-4825-87ab-eb7119de48b8",
   "metadata": {},
   "source": [
    "### Step 4 - Count\n",
    "\n",
    "To keep things simple in this exercise, we will implement a bigram (*n=2*) model here. Therefor, we have to count all single tokens and pairs in our training data. \n",
    "\n",
    "We approximate the probability of a token following a given token as follows: \n",
    "\n",
    "$$\n",
    "P(w_t \\mid w_{t-1}) \\approx \n",
    "\\frac{\\text{count}(w_{t-1}, w_{t})}\n",
    "     {\\text{count}(w_{t-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "266d1cce-3ed9-4566-a108-a289b0b1c26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'machine': 3, 'learning': 2, 'is': 2, 'awesome': 2, 'i': 1, 'love': 1, 'this': 1})\n",
      "Counter({('machine', 'learning'): 2, ('is', 'awesome'): 2, ('i', 'love'): 1, ('love', 'machine'): 1, ('learning', 'is'): 1, ('this', 'machine'): 1, ('machine', 'is'): 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "\n",
    "def text_pipeline(text: str, vocab: Set[str]) -> List[str]:\n",
    "    return map_unk(tokenize(text), vocab)\n",
    "\n",
    "def count_tokens(corpus: List[str], vocab: Set[str]) -> Counter:\n",
    "    counts = Counter()\n",
    "    for text in corpus:\n",
    "        for token in text_pipeline(text, vocab):\n",
    "            counts[token] += 1\n",
    "    return counts\n",
    "\n",
    "def count_pairs(corpus: List[str], vocab: Set[str]) -> Counter:\n",
    "    pairs = Counter()\n",
    "    for text in corpus:\n",
    "        tokens = text_pipeline(text, vocab)\n",
    "        for i in range(1, len(tokens)):\n",
    "            prev, curr = tokens[i-1], tokens[i]\n",
    "            pairs[(prev, curr)] += 1\n",
    "    return pairs\n",
    "\n",
    "print(count_tokens(corpus, vocab))\n",
    "print(count_pairs(corpus,vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e11353-9999-41b8-be37-c06cd9598cf1",
   "metadata": {},
   "source": [
    "### Step 5 - Predicts\n",
    "\n",
    "Great, now we have everything set up to make our prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4567affd-5eb4-413e-8702-bdc8cc27c2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\n"
     ]
    }
   ],
   "source": [
    "def predict_next(prev: str, vocab: Set[str],\n",
    "                 tokens: Counter, pairs: Counter) -> str | None:\n",
    "    # find all followers of prev\n",
    "    followers = [w for (p, w) in pairs if p == prev]\n",
    "    if not followers:\n",
    "        return None\n",
    "\n",
    "    prev_count = tokens[prev]  # denominator\n",
    "\n",
    "    # compute probabilities\n",
    "    probs = {w: pairs[(prev, w)] / prev_count for w in followers}\n",
    "\n",
    "    # return the follower with highest probability\n",
    "    return max(probs, key=probs.get)\n",
    "\n",
    "vocab = build_vocab(corpus)\n",
    "tokens = count_tokens(corpus, vocab)\n",
    "pairs  = count_pairs(corpus, vocab)\n",
    "\n",
    "print(predict_next(\"machine\", vocab, tokens, pairs)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
