{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851618ee-9b23-4523-af66-2fd496e0c64d",
   "metadata": {},
   "source": [
    "# Word2Vec (PyTorch Guide)\n",
    "\n",
    "Word2Vec is a set of techniques for representing words as numerical vectors.  \n",
    "These vectors are positioned in a high-dimensional space so that **similar words are close together**.  \n",
    "These vectors are called **word embeddings**.\n",
    "\n",
    "For example:  \n",
    "- Words like *\"king\"* and *\"queen\"* will have vectors near each other.  \n",
    "- A word like *\"book\"* will be positioned farther away.  \n",
    "- Even simple vector arithmetic works, e.g., **king − man + woman ≈ queen**.  \n",
    "\n",
    "This representation allows machine learning models to capture **semantic relationships** between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dec867-eec1-4c4f-9b71-913a5391c2c5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Step 1: Familiarize with training data.\n",
    "\n",
    "In this notebook, we use 100 sentences i scraped from redit about ML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3227b6ac-c82d-4eca-bb9d-c7e36e53c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence\n",
      "0                 Make computer understand patterns.\n",
      "1                      It's glorified curve fitting.\n",
      "2  Algorithms that make predictions based on prev...\n",
      "3  Machine Learning is using data to answer quest...\n",
      "4  Iterative problem solving using computers for ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../datasets/ML_Sentences.csv\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567db2ae-59ca-4477-8305-6848e0e78730",
   "metadata": {},
   "source": [
    "### Step 2: Build the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78082242-9710-427c-bcb2-62db010ba51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'wish', 'i', 'were', 'an', 'ml', 'expert', '.']\n",
      "[17, 0, 17, 0, 22, 472, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# load a simple tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# the data must be an iterable of strings\n",
    "def tokenize(sentences):\n",
    "    for s in sentences:\n",
    "        yield tokenizer(s)\n",
    "sentences = df[\"sentence\"].astype(str).tolist()\n",
    "tokenized_data = tokenize(sentences)\n",
    "\n",
    "# build vocabulary \n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(tokenized_data, specials=['unk'])\n",
    "vocab.set_default_index(vocab[\"unk\"])\n",
    "\n",
    "# demonstration\n",
    "sample = \"I wish I were an ML expert.\" \n",
    "tokens = tokenizer(sample)\n",
    "print(tokens)\n",
    "\n",
    "# you can write a function to translate tokens via vocab into numbers\n",
    "text_pipeline = lambda tokens:[vocab[token] for token in tokens]\n",
    "print(text_pipeline(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d675a-ccf6-4341-a1c3-f169c1ab8471",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (CBOW)\n",
    "\n",
    "The **Continuous Bag of Words (CBOW)** model predicts a **target word** from a fixed-size **context window** of surrounding words. The context consists of the words that appear before and after the target.\n",
    "\n",
    "**Example:**  \n",
    "Sentence:  \n",
    "`I wish I were an ML expert`  \n",
    "\n",
    "With a **context window size of 2**, the context for predicting `I` is:  `[\"I\", \"wish\", \"were\", \"an\"]`.\n",
    "\n",
    "**Training Data:**  \n",
    "The training data is structured as pairs **(x, y)**, where:  \n",
    "- **x** is the input context: $(w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2})$\n",
    "- **y** is the target word to predict: $w_t$\n",
    "\n",
    "The model learns to estimate:  \n",
    "$$\n",
    "P(w_t \\mid w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "796158d3-0cff-472b-b73a-3de3b2053499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('make', 'computer', 'patterns', '.'), 'understand'), (('it', \"'\", 'glorified', 'curve'), 's'), ((\"'\", 's', 'curve', 'fitting'), 'glorified'), (('s', 'glorified', 'fitting', '.'), 'curve'), (('algorithms', 'that', 'predictions', 'based'), 'make')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_WINDOW = 2\n",
    "\n",
    "def setup_training_data(tokenized_sentences):\n",
    "    training_data = []\n",
    "    for s in tokenized_sentences:\n",
    "        for i in range(CONTEXT_WINDOW, len(s) - CONTEXT_WINDOW):\n",
    "            wtm2, wtm1 = s[i-2], s[i-1]\n",
    "            wta1, wta2 = s[i+1], s[i+2]\n",
    "            y = s[i]\n",
    "            training_data.append(((wtm2, wtm1, wta1, wta2), y))\n",
    "    return training_data\n",
    "\n",
    "# Demonstration\n",
    "training_data = setup_training_data(tokenize(sentences))\n",
    "print(training_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c35a78-7b80-4bc7-9d84-cb248f2b3e85",
   "metadata": {},
   "source": [
    "The `collate_batch` function prepares training batches by converting each `(context, target)` pair into numerical representations using the vocabulary.  \n",
    "The function should return **two tensors**:  \n",
    "- A **context tensor** of shape `(batch_size, context_window * 2)`  \n",
    "- A **target tensor** of shape `(batch_size)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98fb10a7-6161-4bb5-8750-1734834ae390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Device for training: use GPU (CUDA) if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    context_list, target_list = [], []\n",
    "    for context, target in batch:\n",
    "        _context = torch.tensor(text_pipeline(context), dtype=torch.int64)\n",
    "        context_list.append(_context)\n",
    "        target_list.append(vocab[target])\n",
    "\n",
    "    context_tensor = torch.cat(context_list).to(device)\n",
    "    target_tensor = torch.tensor(target_list, dtype=torch.int64).to(device)\n",
    "    \n",
    "    return context_tensor, target_tensor\n",
    "\n",
    "# Demonstration\n",
    "context_tensor, target_tensor = collate_batch(training_data[:5])\n",
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becad71c-fb94-4b0c-9dd8-41e310556a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
