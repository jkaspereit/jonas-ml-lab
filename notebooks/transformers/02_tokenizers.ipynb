{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39b2a34-fcad-4941-8f66-09fa7943bcb9",
   "metadata": {},
   "source": [
    "# üìù Tokenization ‚Äî Introduction\n",
    "\n",
    "**Tokenization** is the process of breaking a sentence into smaller pieces, or *tokens*.\n",
    "\n",
    "A **Tokenizer** is a program that splits text into tokens.\n",
    "\n",
    "Tokenizers generate tokens primarily through three main methods:\n",
    "\n",
    "---\n",
    "\n",
    "| **Type**          | **Description**                                                        | **Pros**                                                                                           | **Cons**                                               | **Example**                                   |\n",
    "|-------------------|------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|--------------------------------------------------------|------------------------------------------------|\n",
    "| **Word-based**    | Splits text at spaces/punctuation into full words.                     | Preserves meaning well.                                                                            | Large vocabulary, struggles with rare/unknown words.  | `\"playing\"` ‚Üí `\"playing\"`                     |\n",
    "| **Character-based** | Splits into individual characters.                                    | Very small vocabulary, handles unknown words.                                                      | Loses semantic meaning of whole words.                | `\"playing\"` ‚Üí `\"p\"`, `\"l\"`, `\"a\"`, `\"y\"`, `\"i\"`, `\"n\"`, `\"g\"` |\n",
    "| **Subword-based** | Splits common words whole, breaks rare words into smaller parts.        | Reduces vocabulary size, handles rare/unknown words, and keeps frequent words intact for semantics. | Slightly more complex tokenization process.           | `\"playing\"` ‚Üí `\"play\"`, `\"##ing\"`              |\n",
    "\n",
    "---\n",
    "\n",
    "**Visual Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc629d-48e3-4314-a88d-86b178bdffba",
   "metadata": {},
   "source": [
    "## Word-Based\n",
    "\n",
    "### **NLTK** (Natural Language Toolkit)\n",
    "A classic Python library for teaching and experimenting with Natural Language Processing (NLP). Uses rule-based tokenizers like **Punkt** (needs downloading via `nltk.download()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2fb25e-868e-4b60-88f6-d116196031b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'love', 'playing', 'playful', 'NLP', 'token', 'games', ',', 'do', \"n't\", 'we', '?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jonas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/jonas/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# NTLK doesn't have tokenizer rules built into python code itself, it stores them in seperate files (like a small database).\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "sentence = \"We love playing playful NLP token games, don't we?\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c66795d-2ef0-42f3-a35c-1ada61eab1ae",
   "metadata": {},
   "source": [
    "### **spaCy**\n",
    "Modern, fast, production-ready NLP library. Includes an efficient tokenizer that handles many edge cases automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5640c05-7d6c-44fc-9e99-06e25969287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "['We', 'love', 'playing', 'playful', 'NLP', 'token', 'games', ',', 'do', \"n't\", 'we', '?']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Loads the pre-trained model (en_core_web_sm) and tokenizer rules into the memory. \n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Runs the tokeniser and other components not considered in this notebook (e.g. POS tagging and NER).\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Wrapper Function\n",
    "def spacy_tok(text: str):\n",
    "    return [t.text for t in nlp.make_doc(text)]  # call tokenizer directly\n",
    "\n",
    "tokens = spacy_tok(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f144d-d600-4a70-aeda-066a723de623",
   "metadata": {},
   "source": [
    "### Special Tokens in Word-Based Tokenizers\n",
    "\n",
    "- **`<unk>`** (unknown): Safe handling of **out-of-vocabulary (OOV)** tokens.\n",
    "- **`<pad>`** (padding): Take the longest sequence in a batch, and append **`<pad>`** until sentences have the same length\n",
    "- **`<bos>`** (begin-of-sequence): Tells the decoder when to start.\n",
    "- **`<eos>`** (end-of-sequence): Tells the decoder when to stop.\n",
    "\n",
    "The decoder is the part of the model that generates the output sequence step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e387b5-daad-4601-9d97-7ed32a23d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', 'we', 'love', 'playing', 'playful', 'nlp', 'token', 'games', ',', 'don', \"'\", 't', 'we', '?', '<eos>'], ['<bos>', 'padding', 'example', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "def add_specials(lines, tok, bos=\"<bos>\", eos=\"<eos>\", pad=\"<pad>\"):\n",
    "    seqs = [[bos] + tok(s) + [eos] for s in lines]\n",
    "    max_len = max(len(x) for x in seqs)\n",
    "    return [x + [pad] * (max_len - len(x)) for x in seqs]\n",
    "\n",
    "# tokenizer used inside the helper\n",
    "tok = get_tokenizer(\"basic_english\")  # or: get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "lines = [sentence, \"padding example\"]\n",
    "tokens = add_specials(lines,tok)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e514a-95d5-409c-91d7-7d01ce3ed46b",
   "metadata": {},
   "source": [
    "## **Character-Based**\n",
    "\n",
    "Character-based tokenization splits text into individual characters rather than words or subwords. One example is the **Keras Tokenizer** (from `tensorflow.keras.preprocessing.text`), which can be configured with `char_level=True` to perform this type of tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29c6d75-0fec-487a-903f-604a2b7c12e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['w', 'e', ' ', 'l', 'o', 'v', 'e', ' ', 'p', 'l', 'a', 'y', 'i', 'n', 'g', ' ', 'p', 'l', 'a', 'y', 'f', 'u', 'l', ' ', 'n', 'l', 'p', ' ', 't', 'o', 'k', 'e', 'n', ' ', 'g', 'a', 'm', 'e', 's', ',', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'w', 'e', '?']\n",
      "Token IDs: [8, 2, 1, 3, 5, 12, 2, 1, 6, 3, 7, 9, 13, 4, 10, 1, 6, 3, 7, 9, 14, 15, 3, 1, 4, 3, 6, 1, 11, 5, 16, 2, 4, 1, 10, 7, 17, 2, 18, 19, 1, 20, 5, 4, 21, 11, 1, 8, 2, 22]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Create a character-level tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "\n",
    "# Token IDs\n",
    "ids = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "# Map IDs back to characters\n",
    "reverse_map = {v: k for k, v in tokenizer.word_index.items()}\n",
    "chars = [reverse_map[i] for i in ids]\n",
    "\n",
    "print(\"Characters:\", chars)\n",
    "print(\"Token IDs:\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64d87a-c8d8-4bf9-9593-27087af098b7",
   "metadata": {},
   "source": [
    "**Why is \"W\" = 8?**\n",
    "\n",
    "1. **Build Vocabulary**  \n",
    "   `tokenizer.fit_on_texts([sentence])` scans the text (with `char_level=True`) and collects all unique characters.\n",
    "\n",
    "2. **Assign Index Numbers**  \n",
    "   Each character gets a numeric ID starting at **1**, ordered **by frequency** in the text:  \n",
    "   - Most frequent ‚Üí ID 1  \n",
    "   - Next most frequent ‚Üí ID 2, etc.  \n",
    "   - Rare characters get higher IDs.\n",
    "   - `\"W\"` is 8 because it‚Äôs the 8th most common character in the sentence.\n",
    "\n",
    "3. **Convert Text ‚Üí Sequences**  \n",
    "   `tokenizer.texts_to_sequences([sentence])` replaces each character with its assigned ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e360463e-a6a3-4fd5-b33a-44f7adfa2f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent character (ID 1): ' '\n",
      "Second most frequent character (ID 2): 'e'\n",
      "Least frequent character (highest ID 22): '?'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Most frequent character (ID 1): '{reverse_map[1]}'\")\n",
    "print(f\"Second most frequent character (ID 2): '{reverse_map[2]}'\")\n",
    "print(f\"Least frequent character (highest ID {len(reverse_map)}): '{reverse_map[len(reverse_map)]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f18ed-896e-45a4-a462-9fbc6b5cfdef",
   "metadata": {},
   "source": [
    "## Subword-Based\n",
    "\n",
    "### WordPiece\n",
    "\n",
    "WordPice is a **subword tokenization algorithm** developed for speech recognition at **Google** and later used in **BERT**. \n",
    "\n",
    "**How it works**\n",
    "\n",
    "1. It start small, the initial vocuabulary includes every charachter that appear in the draining data.\n",
    "2. It learns iteratively and merges vocabulary to repesent the training text until it hits the target vocubulary size (e.g., 30k)\n",
    "\n",
    "**Example**\n",
    "Training text: \"playing\"\n",
    "\n",
    "Start vocab: ['p', 'l', 'a', 'y', 'i', 'n', 'g']\n",
    "\n",
    "Merge pairs if it improves likelihood:\n",
    "\n",
    "Maybe merge \"p\" + \"l\" ‚Üí \"pl\"\n",
    "\n",
    "Later merge \"play\" + \"ing\"\n",
    "\n",
    "Final tokenization might be:\n",
    "\"playing\" ‚Üí [\"play\", \"##ing\"]\n",
    "\n",
    "\\## means ‚Äúcontinuation of a previous token‚Äù in BERT‚Äôs WordPiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b537c637-9e4f-4afc-9ab7-995a32a20583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'love',\n",
       " 'playing',\n",
       " 'playful',\n",
       " 'nl',\n",
       " '##p',\n",
       " 'token',\n",
       " 'games',\n",
       " ',',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'we',\n",
       " '?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df84fa-3b17-4371-a49e-7a639f185944",
   "metadata": {},
   "source": [
    "Right now \"playing\" appears as a whole token because the BERT vocabulary already contains it.\n",
    "WordPiece only breaks a word into subwords (e.g., play + ##ing) when that exact word is not in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1560e42e-d991-4dc6-8d3c-294814963e45",
   "metadata": {},
   "source": [
    "### Unigram\n",
    "\n",
    "**Unigram** is a **subword tokenization algorithm** that starts with a large vocabulary and gradually removes pieces that contribute the least to representing the training data.  \n",
    "\n",
    "**How it works (Unigram)**\n",
    "\n",
    "1. Start with a **large candidate vocabulary** (all characters + many possible substrings).\n",
    "2. Assign each piece a **probability** of appearing in the text.\n",
    "3. Iteratively **prune** the least useful pieces (those whose removal least hurts the model‚Äôs likelihood).\n",
    "4. Stop when you reach the **target vocabulary size** (e.g., 32k).\n",
    "\n",
    "**Example**\n",
    "Training word: `\"playing\"`\n",
    "\n",
    "Initial vocab:  \n",
    "`['p', 'l', 'a', 'y', 'i', 'n', 'g', 'pl', 'play', 'ing']`\n",
    "\n",
    "Step 1: Assign probabilities based on training data usage.  \n",
    "Step 2: Remove rare or low-probability pieces (e.g., `'pl'` if it‚Äôs rarely used).  \n",
    "Step 3: Keep high-probability pieces like `'play'` and `'ing'`.\n",
    "\n",
    "Final tokenization might be:  \n",
    "`\"playing\"` ‚Üí `['play', 'ing']`\n",
    "\n",
    "**SentencePiece** is the framework/tool that implements tokenization algorithms like Unigram and a required dependency for the XLNetTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c60078-e93c-4a57-8362-2a6bcbbad355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅWe',\n",
       " '‚ñÅlove',\n",
       " '‚ñÅplaying',\n",
       " '‚ñÅplayful',\n",
       " '‚ñÅN',\n",
       " 'LP',\n",
       " '‚ñÅtoken',\n",
       " '‚ñÅgames',\n",
       " ',',\n",
       " '‚ñÅdon',\n",
       " \"'\",\n",
       " 't',\n",
       " '‚ñÅwe',\n",
       " '?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "xlnet_tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2155937-d39b-4740-b2b6-9c26163283dd",
   "metadata": {},
   "source": [
    "Tokens are prefixed with \"‚ñÅ\" to indicate they are new words preceded by a space in the original text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1ed5b-c81a-44ce-9944-39b4e799646a",
   "metadata": {},
   "source": [
    "# üìä Tokenization Performance Analysis\n",
    "\n",
    "In this section, we evaluate and compare the tokenization capabilities of four different NLP libaries (nltk, spaCy, BertTokenizer, XLNetTokenizer) by analyzing the frequenzy of tokenized words and measuring the processing time for each tool using datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7709390-f076-4ebf-bc48-2b853e600773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== NLTK word_tokenize ==\n",
      "time: 1.74 ms | tokens: 158 | unique: 107\n",
      "top 10: [('.', 12), ('and', 9), (',', 8), (':', 6), (\"n't\", 5), ('vs', 4), ('?', 3), ('me', 3), ('@', 3), ('playing', 2)]\n",
      "\n",
      "== spaCy en_core_web_sm ==\n",
      "time: 3.77 ms | tokens: 165 | unique: 110\n",
      "top 10: [('.', 12), ('and', 9), (',', 8), (\"n't\", 5), (':', 5), ('-', 4), ('vs', 4), ('?', 3), ('‚Äî', 3), ('me', 3)]\n",
      "\n",
      "== BERT WordPiece ==\n",
      "time: 4.72 ms | tokens: 247 | unique: 147\n",
      "top 10: [('.', 22), (',', 10), (\"'\", 9), ('and', 9), (':', 6), ('token', 5), ('t', 5), ('-', 5), ('vs', 4), ('/', 4)]\n",
      "\n",
      "== XLNet Unigram ==\n",
      "time: 1.82 ms | tokens: 273 | unique: 162\n",
      "top 10: [('.', 22), ('‚ñÅ', 15), (',', 9), (\"'\", 9), ('s', 9), ('‚ñÅand', 9), ('t', 6), ('-', 5), (':', 5), ('‚ñÅtoken', 4)]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "test_corpus = [\n",
    "    \"We love playing playful NLP token games, don't we?\",\n",
    "    \"Tokenization isn't magic‚Äîit's rules, data, and edge cases.\",\n",
    "    \"Re-enter vs reenter vs re-enter: which one splits?\",\n",
    "    \"Email me at alice.bob@example.co.uk or visit https://example.org.\",\n",
    "    \"Caf√© M√ºnster costs ‚Ç¨3.50 ‚Äî deal?\",\n",
    "    \"I‚Äôm not mad‚Äîjust surprised üòÑ.\",\n",
    "    \"IBM taught me tokenization; XLNet taught me SentencePiece.\",\n",
    "    \"New words emerge daily: doomscrolling, micro-SaaS, and finfluencers.\",\n",
    "    \"Version v2.1.0-alpha+build.7 fixed 12/08/2025 bugs.\",\n",
    "    \"Numbers: 1,234,567 and 3.14 and 0.001%.\",\n",
    "    \"Hashtags and mentions: #NLP @you @OpenAI\",\n",
    "    \"Quotes: ‚Äúsmart‚Äù vs 'dumb' and ASCII vs UTF-8.\",\n",
    "    \"Unicorns play, playing and replaying token games.\",\n",
    "    \"‰∏≠ÊñáÂ≠óÁ¨¶Ê∑∑Âêà with English tokens.\",\n",
    "    \"Pok√©mon and M√ºnchen are tricky for lowercasing.\",\n",
    "    \"Let's test don't, couldn't, and shouldn't together.\"\n",
    "]\n",
    "    \n",
    "def run_test(tokenize_fn, lines, name=\"tokenizer\"):\n",
    "    start = datetime.now()\n",
    "    all_tokens = []\n",
    "    for line in lines:\n",
    "        all_tokens.extend(tokenize_fn(line))\n",
    "    ms = (datetime.now() - start).total_seconds() * 1000.0\n",
    "    freq = Counter(all_tokens)\n",
    "    print(f\"\\n== {name} ==\")\n",
    "    print(f\"time: {ms:.2f} ms | tokens: {len(all_tokens)} | unique: {len(freq)}\")\n",
    "    print(\"top 10:\", freq.most_common(10))\n",
    "    return ms, freq\n",
    "\n",
    "tokenizers = [\n",
    "    (\"NLTK word_tokenize\", nltk.word_tokenize),\n",
    "    (\"spaCy en_core_web_sm\", spacy_tok),\n",
    "    (\"BERT WordPiece\", bert_tokenizer.tokenize),\n",
    "    (\"XLNet Unigram\", xlnet_tokenizer.tokenize)\n",
    "]\n",
    "\n",
    "for name, fn in tokenizers:\n",
    "    run_test(fn,test_corpus,name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
