{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a023aa84-3073-47e5-a844-09ae4a5a9367",
   "metadata": {},
   "source": [
    "## **Description**\n",
    "\n",
    "This Jupyter Notebook is a step-by-step tutorial that teaches you how to build your own **chatbot** using the **Hugging Face Transformers** library and a **Seq2Seq (sequence-to-sequence) model**. The goal is to help you understand both the **technical concepts** and the **practical implementation**, so you can adapt this code for your own chatbot projects.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is a Seq2Seq Model?**\n",
    "\n",
    "A **Seq2Seq model** is a type of neural network designed to transform one sequence of data into another. It is widely used in:\n",
    "\n",
    "* **Machine Translation** (e.g., English → German)\n",
    "* **Summarization** (e.g., long text → short summary)\n",
    "* **Chatbots and Dialogue Systems** (e.g., user input → model-generated reply)\n",
    "\n",
    "Seq2Seq models work in two main stages:\n",
    "\n",
    "1. **Encoder** – Reads and processes the input sequence, converting it into a fixed-size vector representation.\n",
    "2. **Decoder** – Takes this vector and generates the output sequence step-by-step.\n",
    "\n",
    "Modern Seq2Seq models often use the **Transformer architecture**, which replaces older recurrent networks (like LSTMs) with attention mechanisms for better efficiency and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Technical Concepts Covered**\n",
    "\n",
    "#### 1. **Transformers Library**\n",
    "\n",
    "The code uses Hugging Face’s `transformers` package, which provides pre-trained models and tokenizers for various NLP tasks.\n",
    "\n",
    "* `AutoTokenizer` – Converts text into numerical tokens (and back to text).\n",
    "* `AutoModelForSeq2SeqLM` – Loads a pre-trained Seq2Seq model for text generation tasks.\n",
    "\n",
    "#### 2. **Tokenization**\n",
    "\n",
    "Before a model can process text, it needs to be converted into **tokens** (integers representing words or subwords).\n",
    "\n",
    "* `tokenizer.encode()` transforms user input into token IDs.\n",
    "* `tokenizer.decode()` converts generated token IDs back into human-readable text.\n",
    "\n",
    "#### 3. **Model Inference**\n",
    "\n",
    "The chatbot uses:\n",
    "\n",
    "```python\n",
    "model.generate(inputs, max_new_tokens=250)\n",
    "```\n",
    "\n",
    "This instructs the model to **predict** the next tokens in the response until a stopping point is reached or the maximum token limit is hit.\n",
    "\n",
    "#### 4. **Interactive Chat Loop**\n",
    "\n",
    "The `while True` loop creates a real-time conversation:\n",
    "\n",
    "* User types a message (`input()`).\n",
    "* The model generates a response.\n",
    "* Conversation continues until the user types `\"exit\"`, `\"quit\"`, or `\"stop\"`.\n",
    "\n",
    "#### 5. **Stopping Conditions**\n",
    "\n",
    "A simple condition checks if the user wants to end the chat:\n",
    "\n",
    "```python\n",
    "if input_text.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **How the Code Works Step-by-Step**\n",
    "\n",
    "1. **Load Model and Tokenizer**\n",
    "\n",
    "   ```python\n",
    "   model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "   ```\n",
    "   \n",
    "\n",
    "2. **Wait for User Input**\n",
    "   The chatbot waits for your message with:\n",
    "\n",
    "   ```python\n",
    "   input_text = input(\"You: \")\n",
    "   ```\n",
    "   \n",
    "\n",
    "3. **Tokenize the Input**\n",
    "\n",
    "   ```python\n",
    "   inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "   ```\n",
    "   \n",
    "\n",
    "4. **Generate a Response**\n",
    "\n",
    "   ```python\n",
    "   outputs = model.generate(inputs, max_new_tokens=250)\n",
    "   ```\n",
    "   \n",
    "\n",
    "5. **Decode the Output**\n",
    "\n",
    "   ```python\n",
    "   response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "   ```\n",
    "   \n",
    "\n",
    "6. **Display the Response**\n",
    "\n",
    "   ```python\n",
    "   print(\"Chatbot:\", response)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Goals**\n",
    "\n",
    "By following this notebook, you will learn:\n",
    "\n",
    "* What a Seq2Seq model is and how it works.\n",
    "* How to load and use pre-trained Hugging Face models.\n",
    "* How to tokenize and detokenize text for model input/output.\n",
    "* How to create an interactive chatbot loop in Python.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also **extend this description** with diagrams showing the **encoder-decoder process** for Seq2Seq, so your notebook looks even more visually engaging. That would make the explanation much clearer for beginners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd29429-111d-4bb0-bc5e-5aab6071bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def chat_with_model(model_name):\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    while True: \n",
    "        input_text = input(\"You: \") \n",
    "\n",
    "        if input_text.lower() in [\"exit\",\"quit\",\"stop\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        inputs = tokenizer.encode(input_text,return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs,max_new_tokens=250)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426daa6-c6ac-4ec0-87f5-4ea0f96b5085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Is the sky blue?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  What color is the sky? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: blue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Is the color of the sky blue?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: no\n"
     ]
    }
   ],
   "source": [
    "chat_with_model(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34148aa6-9466-4f87-9913-efc90905d145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
